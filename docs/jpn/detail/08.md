# ⑧公平性の原則

## a-ＡＩの学習等に用いられるデータの代表性への留意

### 解説

AIサービスプロバイダ、ビジネス利用者及びデータ提供者は、
AIの判断が学習時のデータによって決定づけられる可能性があることを踏まえ、
AIを利活用する際の社会的文脈に応じて、以下のとおり、
**AIの学習等に用いられるデータの代表性`1`やデータに内在する社会的なバイアス等**に留意することが期待される。

#### 公平性の観点からAIの学習等を行う際に留意すべき事項（例）
* 不公平な判断が行われることのないようアルゴリズムが設計されていても、
データの代表性が確保されないことによってバイアスが生じうることへの留意。
* センシティブ情報`2`を含む場合`3`に加え、センシティブ情報が含まれていない場合であっても、
社会的バイアスを内在するデータを用いることによってバイアスが生じる可能性があること`4`への留意。
* （教師あり学習の場合）データの前処理において、学習データのラベルは多くの場合
人間が作成・付与するため、（意図的にまたは意図せずに）ラベル付与を行う人のバイアスが入り込むことへの留意`4`。
* データの代表性を満足するためにパーソナルデータを含む
大量のデータを集めようとする場合において、データに含まれる個人のプライバシーの尊重。

----
* `1`) データの「代表性」とは、サンプルとして抽出され利活用に供されているデータが、その母集団の性質を正確に反映している度合いのことをいう。
* `2`) 公平性の観点から排除すべき対象者の性別や人種等の個人の属性に関する情報。
* `3`) センシティブ情報を含むデータを用いて学習等を行う場合に公平性を確保するための基準が検討されており、
その一部を[⑧-b：学習アルゴリズムによるバイアスへの留意](#b-学習アルゴリズムによるバイアスへの留意)の公平性の基準（例）に掲載している。
* `4`) 例えば、性別に依存しない採用試験を行おうとした場合に、仮に特定の項目に対する割合が男女間で相当程度異なるとすると、
当該項目を一属性に加えて採用試験を行うアルゴリズムにより、結果として性別によるバイアスが生じる可能性がある。
また、ある集団ではセンシティブではないものが、別の集団ではセンシティブなものとして扱われるなど、
国・地域などの集団ごとの社会通念の差異により、バイアスが生じる可能性があることにも留意が必要である。
* `5`) 対策として、ラベリングの統一的な基準を作ることも考えられる。


----
### 参考

消費者的利用者は、AIの判断結果について疑義を感じた場合には、
必要に応じて、開発者、AIサービスプロバイダ、ビジネス利用者等に問い合わせを行うことが望ましい。

****************

* [トップページ](../../)

****************


## b-学習アルゴリズムによるバイアスへの留意

### 解説
AIサービスプロバイダ及びビジネス利用者は、AIに用いられる学習アルゴリズムにより、AIの判断にバイアスが生じる可能性があることに留意することが期待される。
特に、機械学習においては、一般的に、多数派がより尊重され、少数派が反映されにくい傾向にあり（バンドワゴン効果）、
この課題を回避するため、例えば**以下の方法**が考えられる。


#### 機械学習アルゴリズムによるバイアスを生じさせないための方法（例）
* AIを利活用する際の社会的文脈を踏まえ、センシティブ属性（公平性の観点から排除すべき対象者の性別や人種等の個人の属性）を明確化する`1`。
* センシティブ属性に関し確保すべき公平性の内容を、例えば以下の基準のとおり明確化する。
* 上記の公平性を満たす制約を機械学習アルゴリズムに付加する。
* ただし、（アルゴリズムにもよるが）公平性について上記の制約を課すことにより、機械学習の精度に影響を及ぼす可能性がある。


#### 公平性の基準（例）`2`
##### 集団公平性
* センシティブ属性を取り除き、非センシティブ属性のみに基づき予測を行う（unawareness）。
* センシティブ属性の値が異なる複数のグループ間で、同じ予測結果を確保する（demographic parity）。
* 実際の結果に対する予測結果の誤差の比率を、センシティブ属性の値によらないように調整する（equalized odds）。
##### 個人公平性
* センシティブ属性以外の属性値が等しい個人に対してはそれぞれ同じ予測結果を与える。
* 類似した属性値を持つ個人には類似した予測結果を与える（Fairness through awareness）。


----

* `1`) 例えば、入社試験で個人の属性情報から採否を判断する場合を考える。性別に依存することが問題視されている場合には、性別がセンシティブ属性となる。
* `2`) 上記1の例で、（機械学習）アルゴリズムにより採否を予測するとして、
  * unawareness：性別に関する属性を取り除いてアルゴリズムを適用すること。
  * demographic parity：アルゴリズムによる採否予測の分布（比率等）を男女間で同一となるよう調整すること。
  * equalized odds：実データの男女間の採否の分布（比率等）がアルゴリズムによる男女間の採否予測の分布（比率等）と同一になるよう調整すること。

----

### 参考

消費者的利用者は、
AIの判断結果について疑義を感じた場合には、必要に応じて、開発者、AIサービスプロバイダ、ビジネス利用者等
に問い合わせを行うことが望ましい。

****************

* [トップページ](../../)

****************




## c-人間の判断の介在（公平性の確保）

### 解説
AIサービスプロバイダ及びビジネス利用者は、AIによりなされた判断結果の公平性`1`を保つため、
AIを利活用する際の社会的文脈や人々の合理的な期待を踏まえ、その判断を用いるか否か、
あるいは、どのように用いるか等に関し、人間の判断を介在させることが期待される。
人間の判断の介在の要否については、[①－b](./01.md#b-人間の判断の介在) に掲げる内容を参照しつつ、
公平性の観点から、例えば以下の基準も踏まえ、検討することが期待される。

#### 人間の判断の介在の要否について、基準として考えられる観点（例）
* 統計的な将来予測が（不確定性が高く）難しい場合`2`。
* 意思決定（判断）に対し納得ある理由を必要とする場合`3`。
* 学習データにマイノリティなどに対するバイアスが含まれていること等により、人種・信条・性別に基づく差別が想定される場合。

----

* `1`) AIの学習に用いるデータに内在する社会的バイアス等がAIによる判断結果の公平性に影響を及ぼしうることを前提としている。
* `2`) 例えば、人事においては、従業員の能力や生産性といった時間とともに変動する変数が用いられること、また、記録として残らない情報は用いることができないこと等から、統計的な将来予測が難しい。
* `3`) 例えば、人事評価に当たっては、社員に対し評価の理由を説明できることが期待される。

----


****************

* [トップページ](../../)

****************

